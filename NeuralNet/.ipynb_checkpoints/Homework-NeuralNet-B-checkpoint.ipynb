{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Neural networks - Part B (55 points)\n",
    "## Gradient descent for simple two and three layer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "This homework is due before midnight on Monday, Feb. 13, 2023.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this assignment implements the gradient descent algorithm for a simple artificial neuron. The second part implements backpropagation for a simple network with one hidden unit.\n",
    "\n",
    "In the first part, the neuron will learn to compute logical OR. The neuron model and logical OR are shown below, for inputs $x_0$ and $x_1$ and target output $y$.\n",
    "\n",
    "<img src=\"images/nn_OR.jpeg\" style=\"width: 350px;\"/>\n",
    "\n",
    "This assignment requires some basic PyTorch knowledge. You can review your notes from lab and this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). The \"Introduction to PyTorch\" section on the PyTorch website is also helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `torch.tensor` objects for representing the data matrix `X` with targets `Y_or` (for the logical OR function). Each row of `X` is a different input pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor X:\n",
      "  has shape torch.Size([4, 2])\n",
      "  and contains tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "Target tensor Y:\n",
      "  has shape torch.Size([4])\n",
      "  and contains tensor([0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "X_list = [[0.,0.], [0.,1.], [1.,0.], [1.,1.]]\n",
    "X = torch.tensor(X_list)\n",
    "Y_or = torch.tensor([0.,1.,1.,1.])\n",
    "N = X.shape[0] # number of input patterns\n",
    "print(\"Input tensor X:\")\n",
    "print('  has shape',X.shape)\n",
    "print('  and contains',X)\n",
    "print('Target tensor Y:')\n",
    "print('  has shape',Y_or.shape)\n",
    "print('  and contains',Y_or)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artificial neuron operates as follows. Given an input vector $x$ (which is one row of input tensor $X$), the net input ($\\textbf{net}$) to the neuron is computed as follows\n",
    "\n",
    "$$ \\textbf{net} = \\sum_i x_i w_i + b,$$\n",
    "\n",
    "for weights $w_i$ and bias $b$. The activation function $g(\\textbf{net})$ is the logistic function\n",
    "\n",
    "$$ g(\\textbf{net}) = \\frac{1}{1+e^{-\\textbf{net}}},$$\n",
    "\n",
    "which is used to compute the predicted output $\\hat{y} = g(\\textbf{net})$. Finally, the loss (squared error) for a particular pattern $x$ is defined as \n",
    "\n",
    "$$ E(w,b) = (\\hat{y}-y)^2,$$\n",
    "\n",
    "where the target output is $y$. **Your main task is to manually compute the gradients of the loss $E$ with respect to the neuron parameters:**\n",
    "\n",
    "$$\\frac{\\partial E(w,b)}{\\partial w}, \\frac{\\partial E(w,b)}{\\partial b}.$$\n",
    "\n",
    "By manually, we mean to program the gradient computation directly, using the formulas discussed in class. This is in contrast to using PyTorch's `autograd` (Automatric differentiation) that computes the gradient automatically, as discussed in class, lab, and in the PyTorch tutorial (e.g., `loss.backward()`). First, let's write the activation function and the loss in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_logistic(net):\n",
    "    return 1. / (1.+torch.exp(-net))\n",
    "\n",
    "def loss(yhat,y):\n",
    "    return (yhat-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll also write two functions for examining the internal operations of the neuron, and the gradients of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_forward(x,yhat,y):\n",
    "    # Examine network's prediction for input x\n",
    "    print(' Input: ',end='')\n",
    "    print(x.numpy())\n",
    "    print(' Output: ' + str(round(yhat.item(),3)))\n",
    "    print(' Target: ' + str(y.item()))\n",
    "\n",
    "def print_grad(grad_w,grad_b):\n",
    "    # Examine gradients\n",
    "    print('  d_loss / d_w = ',end='')\n",
    "    print(grad_w)\n",
    "    print('  d_loss / d_b = ',end='')\n",
    "    print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dive in and begin the implementation of stochastic gradient descent. We'll initialize our parameters $w$ and $b$ randomly, and proceed through a series of epochs of training. Each epoch involves visiting the four training patterns in random order, and updating the parameters after each presentation of an input pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (10 points) </h3>\n",
    "<br>\n",
    "In the code below, fill in code to manually compute the gradient in closed form.\n",
    "    <ul>\n",
    "        <li>See lecture slides for the equation for the gradient for the weights w.</li>\n",
    "        <li>Derive (or reason) to get the equation for the gradient for bias b.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (5 points) </h3>\n",
    "<br>\n",
    "In the code below, fill in code for the weight and bias update rule for gradient descent.\n",
    "</div>\n",
    "\n",
    "After completing the code, run it to compare **your gradients** with the **ground-truth computed by PyTorch.** (There may be small differences that you shouldn't worry about, e.g. within 1e-6). Also, you can check the neuron's performance at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the gradient manually\n",
      " Input: [0. 0.]\n",
      " Output: 0.122\n",
      " Target: 0.0\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.02628346]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.02628346]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 0.]\n",
      " Output: 0.047\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-0.08472404 -0.        ]\n",
      "  d_loss / d_b = [-0.08472404]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [-0.08472405  0.        ]\n",
      "  d_loss / d_b = [-0.08472405]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 1.]\n",
      " Output: 0.064\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-0.11202463 -0.11202463]\n",
      "  d_loss / d_b = [-0.11202463]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [-0.11202462 -0.11202462]\n",
      "  d_loss / d_b = [-0.11202462]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [0. 1.]\n",
      " Output: 0.164\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-0.         -0.22906815]\n",
      "  d_loss / d_b = [-0.22906815]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [ 0.         -0.22906813]\n",
      "  d_loss / d_b = [-0.22906813]\n",
      "\n",
      "epoch 0; error=2.499\n",
      "epoch 50; error=0.721\n",
      "epoch 100; error=0.39\n",
      "epoch 150; error=0.316\n",
      "epoch 200; error=0.269\n",
      "epoch 250; error=0.233\n",
      "epoch 300; error=0.205\n",
      "epoch 350; error=0.182\n",
      "epoch 400; error=0.163\n",
      "epoch 450; error=0.148\n",
      "epoch 500; error=0.134\n",
      "epoch 550; error=0.123\n",
      "epoch 600; error=0.114\n",
      "epoch 650; error=0.105\n",
      "epoch 700; error=0.098\n",
      "epoch 750; error=0.091\n",
      "epoch 800; error=0.086\n",
      "epoch 850; error=0.08\n",
      "epoch 900; error=0.076\n",
      "epoch 950; error=0.072\n",
      "epoch 1000; error=0.068\n",
      "epoch 1050; error=0.064\n",
      "epoch 1100; error=0.061\n",
      "epoch 1150; error=0.059\n",
      "epoch 1200; error=0.056\n",
      "epoch 1250; error=0.053\n",
      "epoch 1300; error=0.051\n",
      "epoch 1350; error=0.049\n",
      "epoch 1400; error=0.047\n",
      "epoch 1450; error=0.045\n",
      "epoch 1500; error=0.044\n",
      "epoch 1550; error=0.042\n",
      "epoch 1600; error=0.041\n",
      "epoch 1650; error=0.039\n",
      "epoch 1700; error=0.038\n",
      "epoch 1750; error=0.037\n",
      "epoch 1800; error=0.036\n",
      "epoch 1850; error=0.035\n",
      "epoch 1900; error=0.034\n",
      "epoch 1950; error=0.033\n",
      "epoch 2000; error=0.032\n",
      "epoch 2050; error=0.031\n",
      "epoch 2100; error=0.03\n",
      "epoch 2150; error=0.029\n",
      "epoch 2200; error=0.029\n",
      "epoch 2250; error=0.028\n",
      "epoch 2300; error=0.027\n",
      "epoch 2350; error=0.027\n",
      "epoch 2400; error=0.026\n",
      "epoch 2450; error=0.025\n",
      "epoch 2500; error=0.025\n",
      "epoch 2550; error=0.024\n",
      "epoch 2600; error=0.024\n",
      "epoch 2650; error=0.023\n",
      "epoch 2700; error=0.023\n",
      "epoch 2750; error=0.022\n",
      "epoch 2800; error=0.022\n",
      "epoch 2850; error=0.021\n",
      "epoch 2900; error=0.021\n",
      "epoch 2950; error=0.021\n",
      "epoch 3000; error=0.02\n",
      "epoch 3050; error=0.02\n",
      "epoch 3100; error=0.019\n",
      "epoch 3150; error=0.019\n",
      "epoch 3200; error=0.019\n",
      "epoch 3250; error=0.018\n",
      "epoch 3300; error=0.018\n",
      "epoch 3350; error=0.018\n",
      "epoch 3400; error=0.018\n",
      "epoch 3450; error=0.017\n",
      "epoch 3500; error=0.017\n",
      "epoch 3550; error=0.017\n",
      "epoch 3600; error=0.016\n",
      "epoch 3650; error=0.016\n",
      "epoch 3700; error=0.016\n",
      "epoch 3750; error=0.016\n",
      "epoch 3800; error=0.016\n",
      "epoch 3850; error=0.015\n",
      "epoch 3900; error=0.015\n",
      "epoch 3950; error=0.015\n",
      "epoch 4000; error=0.015\n",
      "epoch 4050; error=0.014\n",
      "epoch 4100; error=0.014\n",
      "epoch 4150; error=0.014\n",
      "epoch 4200; error=0.014\n",
      "epoch 4250; error=0.014\n",
      "epoch 4300; error=0.014\n",
      "epoch 4350; error=0.013\n",
      "epoch 4400; error=0.013\n",
      "epoch 4450; error=0.013\n",
      "epoch 4500; error=0.013\n",
      "epoch 4550; error=0.013\n",
      "epoch 4600; error=0.013\n",
      "epoch 4650; error=0.012\n",
      "epoch 4700; error=0.012\n",
      "epoch 4750; error=0.012\n",
      "epoch 4800; error=0.012\n",
      "epoch 4850; error=0.012\n",
      "epoch 4900; error=0.012\n",
      "epoch 4950; error=0.012\n",
      "Final result:\n",
      " Input: [0. 0.]\n",
      " Output: 0.08\n",
      " Target: 0.0\n",
      "\n",
      "Final result:\n",
      " Input: [0. 1.]\n",
      " Output: 0.95\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 0.]\n",
      " Output: 0.95\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 1.]\n",
      " Output: 1.0\n",
      " Target: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w = torch.randn(2, requires_grad=True) # [size 2] tensor\n",
    "b = torch.randn(1, requires_grad=True) # [size 1] tensor\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 5000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x_pat = X[p,:] # get one input pattern\n",
    "        \n",
    "        # compute output of neuron\n",
    "        net = torch.dot(x_pat,w)+b\n",
    "        yhat = g_logistic(net)\n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_or[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x_pat,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            w_grad = torch.tensor([2*(yhat-y)*g_logistic(net)\n",
    "                                   *(1-g_logistic(net))\n",
    "                                   *x_pat[0],\n",
    "                      2*(yhat-y)*g_logistic(net)\n",
    "                                   *(1-g_logistic(net))\n",
    "                                   *x_pat[1]])\n",
    "            b_grad = torch.tensor([2*(yhat-y)*g_logistic(net)\n",
    "                                   *(1-g_logistic(net))])\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  two lines of the form\n",
    "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
    "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, \n",
    "            #   which we don't want.         \n",
    "            #raise Exception('Replace with your code.')                      \n",
    "        if verbose:\n",
    "            print_grad(w_grad.numpy(),b_grad.numpy())\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
    "            print(\"\")\n",
    "        w.grad.zero_() # clear PyTorch's gradient\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            w -= alpha*w_grad\n",
    "            b -= alpha*b_grad\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            #  two lines of the form:\n",
    "            #    w -=   ....\n",
    "            #    b -=   ....\n",
    "            #raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "\n",
    "# print a final pass through patterns\n",
    "for p in range(X.shape[0]):\n",
    "    x_pat = X[p]\n",
    "    net = torch.dot(x_pat,w)+b\n",
    "    yhat = g_logistic(net)\n",
    "    y = Y_or[p]\n",
    "    print(\"Final result:\")\n",
    "    print_forward(x_pat,yhat,y)\n",
    "    print(\"\")\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (logistic activation)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change the activation function to \"linear\" (identity function) from the \"logistic\" function, such that $g(\\textbf{net}) = \\textbf{net}$. With a linear rather than logistic activation, the output will no longer be constrained between 0 and 1. The artificial neuron will still try to solve the problem with 0/1 targets. Here is the simple implementation of $g(\\cdot)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 3 (5 points) </h3>\n",
    "<br>\n",
    "Just as before, fill in the missing code fragments for implementing gradient descent. This time we are using the linear activation function. Be sure to change your gradient calculation to reflect the new activation function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the gradient manually\n",
      " Input: [0. 0.]\n",
      " Output: 0.775\n",
      " Target: 0.0\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [1.5509496]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [1.5509496]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [0. 1.]\n",
      " Output: 0.369\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-0.        -1.2619509]\n",
      "  d_loss / d_b = [-1.2619509]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [ 0.        -1.2619509]\n",
      "  d_loss / d_b = [-1.2619509]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 0.]\n",
      " Output: 1.584\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [1.1671729 0.       ]\n",
      "  d_loss / d_b = [1.1671729]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [1.1671729 0.       ]\n",
      "  d_loss / d_b = [1.1671729]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 1.]\n",
      " Output: 1.201\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [0.40212798 0.40212798]\n",
      "  d_loss / d_b = [0.40212798]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [0.40212798 0.40212798]\n",
      "  d_loss / d_b = [0.40212798]\n",
      "\n",
      "epoch 0; error=1.38\n",
      "epoch 50; error=0.297\n",
      "epoch 100; error=0.304\n",
      "epoch 150; error=0.308\n",
      "epoch 200; error=0.296\n",
      "epoch 250; error=0.308\n",
      "epoch 300; error=0.304\n",
      "epoch 350; error=0.3\n",
      "epoch 400; error=0.3\n",
      "epoch 450; error=0.301\n",
      "epoch 500; error=0.309\n",
      "epoch 550; error=0.303\n",
      "epoch 600; error=0.303\n",
      "epoch 650; error=0.309\n",
      "epoch 700; error=0.306\n",
      "epoch 750; error=0.3\n",
      "epoch 800; error=0.311\n",
      "epoch 850; error=0.306\n",
      "epoch 900; error=0.31\n",
      "epoch 950; error=0.311\n",
      "epoch 1000; error=0.296\n",
      "epoch 1050; error=0.303\n",
      "epoch 1100; error=0.308\n",
      "epoch 1150; error=0.308\n",
      "epoch 1200; error=0.301\n",
      "epoch 1250; error=0.299\n",
      "epoch 1300; error=0.3\n",
      "epoch 1350; error=0.305\n",
      "epoch 1400; error=0.305\n",
      "epoch 1450; error=0.307\n",
      "epoch 1500; error=0.308\n",
      "epoch 1550; error=0.304\n",
      "epoch 1600; error=0.306\n",
      "epoch 1650; error=0.301\n",
      "epoch 1700; error=0.305\n",
      "epoch 1750; error=0.307\n",
      "epoch 1800; error=0.307\n",
      "epoch 1850; error=0.297\n",
      "epoch 1900; error=0.3\n",
      "epoch 1950; error=0.307\n",
      "epoch 2000; error=0.308\n",
      "epoch 2050; error=0.304\n",
      "epoch 2100; error=0.311\n",
      "epoch 2150; error=0.307\n",
      "epoch 2200; error=0.307\n",
      "epoch 2250; error=0.307\n",
      "epoch 2300; error=0.307\n",
      "epoch 2350; error=0.308\n",
      "epoch 2400; error=0.306\n",
      "epoch 2450; error=0.306\n",
      "epoch 2500; error=0.308\n",
      "epoch 2550; error=0.3\n",
      "epoch 2600; error=0.304\n",
      "epoch 2650; error=0.308\n",
      "epoch 2700; error=0.308\n",
      "epoch 2750; error=0.309\n",
      "epoch 2800; error=0.308\n",
      "epoch 2850; error=0.304\n",
      "epoch 2900; error=0.308\n",
      "epoch 2950; error=0.309\n",
      "epoch 3000; error=0.309\n",
      "epoch 3050; error=0.306\n",
      "epoch 3100; error=0.3\n",
      "epoch 3150; error=0.302\n",
      "epoch 3200; error=0.307\n",
      "epoch 3250; error=0.305\n",
      "epoch 3300; error=0.309\n",
      "epoch 3350; error=0.306\n",
      "epoch 3400; error=0.304\n",
      "epoch 3450; error=0.308\n",
      "epoch 3500; error=0.308\n",
      "epoch 3550; error=0.307\n",
      "epoch 3600; error=0.307\n",
      "epoch 3650; error=0.309\n",
      "epoch 3700; error=0.306\n",
      "epoch 3750; error=0.298\n",
      "epoch 3800; error=0.31\n",
      "epoch 3850; error=0.307\n",
      "epoch 3900; error=0.304\n",
      "epoch 3950; error=0.307\n",
      "epoch 4000; error=0.308\n",
      "epoch 4050; error=0.299\n",
      "epoch 4100; error=0.296\n",
      "epoch 4150; error=0.305\n",
      "epoch 4200; error=0.304\n",
      "epoch 4250; error=0.306\n",
      "epoch 4300; error=0.304\n",
      "epoch 4350; error=0.303\n",
      "epoch 4400; error=0.3\n",
      "epoch 4450; error=0.301\n",
      "epoch 4500; error=0.309\n",
      "epoch 4550; error=0.304\n",
      "epoch 4600; error=0.307\n",
      "epoch 4650; error=0.309\n",
      "epoch 4700; error=0.3\n",
      "epoch 4750; error=0.308\n",
      "epoch 4800; error=0.307\n",
      "epoch 4850; error=0.307\n",
      "epoch 4900; error=0.308\n",
      "epoch 4950; error=0.306\n",
      "Final result:\n",
      " Input: [0. 0.]\n",
      " Output: 0.254\n",
      " Target: 0.0\n",
      "\n",
      "Final result:\n",
      " Input: [0. 1.]\n",
      " Output: 0.751\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 0.]\n",
      " Output: 0.737\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 1.]\n",
      " Output: 1.234\n",
      " Target: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w = torch.randn(2, requires_grad=True) # [size 2] tensor\n",
    "b = torch.randn(1, requires_grad=True) # [size 1] tensor\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 5000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x_pat = X[p,:] # get one input pattern\n",
    "        \n",
    "        # compute output of neuron\n",
    "        net = torch.dot(x_pat,w)+b\n",
    "        yhat = g_linear(net)\n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_or[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x_pat,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            w_grad = torch.tensor([2*(yhat-y)*x_pat[0],\n",
    "                      2*(yhat-y)*x_pat[1]])\n",
    "            b_grad = torch.tensor([2*(yhat-y)])\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  two lines of the form\n",
    "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
    "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.         \n",
    "            #raise Exception('Replace with your code.')                      \n",
    "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
    "            print(\"\")\n",
    "        w.grad.zero_() # clear PyTorch's gradient\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            w -= alpha*w_grad\n",
    "            b -= alpha*b_grad\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            #  two lines of the form:\n",
    "            #    w -=   ....\n",
    "            #    b -=   ....\n",
    "            #raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "\n",
    "# print a final pass through patterns\n",
    "for p in range(X.shape[0]):\n",
    "    x_pat = X[p]\n",
    "    net = torch.dot(x_pat,w)+b\n",
    "    yhat = g_linear(net)\n",
    "    y = Y_or[p]\n",
    "    print(\"Final result:\")\n",
    "    print_forward(x_pat,yhat,y)\n",
    "    print(\"\")\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (linear/null activation)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 4 (10 points) </h3>\n",
    "<br>\n",
    "You'll see above that the artificial neuron, with the simple linear (identity) activation, does worse on the OR problem. Examine the learned weights and bias, and explain why the network does not arrive at a perfect solution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4830, 0.4969], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2537], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the activation function is a simple linear activation, the output is just the dot product of the input and the weights plus the bias. For example, when either $x_0$ or $x_1$ = 1, the output will be equal to $w_i + b$ which is around $0.74$. When $x_0=x_1=0$, the output is equal to $b=0.25$. When $x_0=x_1=1$, the output is around $1.25$. This basically means the model will be a simple linear regression model. Since our ground truth is not linear, a simple linear regression model will not arrive at a perfect solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we have a simple multi-layer network with two input neurons, one hidden neuron, and one output neuron. Both the hidden and output unit should use the logistic activation function. We will learn to compute logical XOR. The network and logical XOR are shown below, for inputs $x_0$ and $x_1$ and target output $y$.\n",
    "\n",
    "<img src=\"images/nn_XOR.jpeg\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 5 (15 points) </h3>\n",
    "<br>\n",
    "You will implement backpropagation for this simple network. In the code below, you have several parts to fill in. First, define the forward pass to compute the output `yhat` from the input `x`. Second, fill in code to manually compute the gradients for all five weights w and two biases b in closed form. Third, fill in the code for updating the biases and weights.\n",
    "</div>\n",
    "\n",
    "After completing the code, run it to compare **your gradients** with the **ground-truth computed by PyTorch.** (There may be small differences that you shouldn't worry about, e.g. within 1e-6). Also, you can check the network's performance at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the gradient manually\n",
      " Input: [1. 1.]\n",
      " Output: 0.297\n",
      " Target: 0.0\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [0.0099684 0.0099684]\n",
      "  d_loss / d_b = [0.0099684]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [0.12383684 0.12383684 0.04397008]\n",
      "  d_loss / d_b = [0.12383684]\n",
      "\n",
      "Compute the gradient using PyTorch .backward()\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [0.0099684 0.0099684]\n",
      "  d_loss / d_b = [0.0099684]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [0.12383683 0.12383683 0.04397008]\n",
      "  d_loss / d_b = [0.12383683]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 0.]\n",
      " Output: 0.445\n",
      " Target: 1.0\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [-0.02252301 -0.        ]\n",
      "  d_loss / d_b = [-0.02252301]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [-0.27401707 -0.         -0.17023158]\n",
      "  d_loss / d_b = [-0.27401707]\n",
      "\n",
      "Compute the gradient using PyTorch .backward()\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [-0.02252301  0.        ]\n",
      "  d_loss / d_b = [-0.02252301]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [-0.27401707  0.         -0.17023158]\n",
      "  d_loss / d_b = [-0.27401707]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [0. 1.]\n",
      " Output: 0.194\n",
      " Target: 1.0\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [-0.         -0.01679811]\n",
      "  d_loss / d_b = [-0.01679811]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [-0.         -0.2522982  -0.06235439]\n",
      "  d_loss / d_b = [-0.2522982]\n",
      "\n",
      "Compute the gradient using PyTorch .backward()\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [ 0.         -0.01679811]\n",
      "  d_loss / d_b = [-0.01679811]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [ 0.         -0.2522982  -0.06235439]\n",
      "  d_loss / d_b = [-0.2522982]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [0. 0.]\n",
      " Output: 0.321\n",
      " Target: 0.0\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.01262444]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [0.         0.         0.06926244]\n",
      "  d_loss / d_b = [0.13991527]\n",
      "\n",
      "Compute the gradient using PyTorch .backward()\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.01262444]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [0.         0.         0.06926244]\n",
      "  d_loss / d_b = [0.13991529]\n",
      "\n",
      "epoch 0; error=1.148\n",
      "epoch 50; error=1.054\n",
      "epoch 100; error=1.039\n",
      "epoch 150; error=1.028\n",
      "epoch 200; error=1.021\n",
      "epoch 250; error=1.017\n",
      "epoch 300; error=1.014\n",
      "epoch 350; error=1.013\n",
      "epoch 400; error=1.011\n",
      "epoch 450; error=1.011\n",
      "epoch 500; error=1.01\n",
      "epoch 550; error=1.009\n",
      "epoch 600; error=1.009\n",
      "epoch 650; error=1.008\n",
      "epoch 700; error=1.008\n",
      "epoch 750; error=1.007\n",
      "epoch 800; error=1.006\n",
      "epoch 850; error=1.005\n",
      "epoch 900; error=1.004\n",
      "epoch 950; error=1.003\n",
      "epoch 1000; error=1.001\n",
      "epoch 1050; error=0.999\n",
      "epoch 1100; error=0.997\n",
      "epoch 1150; error=0.995\n",
      "epoch 1200; error=0.992\n",
      "epoch 1250; error=0.989\n",
      "epoch 1300; error=0.985\n",
      "epoch 1350; error=0.98\n",
      "epoch 1400; error=0.975\n",
      "epoch 1450; error=0.968\n",
      "epoch 1500; error=0.96\n",
      "epoch 1550; error=0.952\n",
      "epoch 1600; error=0.941\n",
      "epoch 1650; error=0.929\n",
      "epoch 1700; error=0.915\n",
      "epoch 1750; error=0.898\n",
      "epoch 1800; error=0.88\n",
      "epoch 1850; error=0.859\n",
      "epoch 1900; error=0.836\n",
      "epoch 1950; error=0.811\n",
      "epoch 2000; error=0.784\n",
      "epoch 2050; error=0.755\n",
      "epoch 2100; error=0.725\n",
      "epoch 2150; error=0.694\n",
      "epoch 2200; error=0.663\n",
      "epoch 2250; error=0.631\n",
      "epoch 2300; error=0.6\n",
      "epoch 2350; error=0.569\n",
      "epoch 2400; error=0.539\n",
      "epoch 2450; error=0.51\n",
      "epoch 2500; error=0.482\n",
      "epoch 2550; error=0.456\n",
      "epoch 2600; error=0.431\n",
      "epoch 2650; error=0.407\n",
      "epoch 2700; error=0.385\n",
      "epoch 2750; error=0.365\n",
      "epoch 2800; error=0.345\n",
      "epoch 2850; error=0.327\n",
      "epoch 2900; error=0.311\n",
      "epoch 2950; error=0.295\n",
      "epoch 3000; error=0.28\n",
      "epoch 3050; error=0.267\n",
      "epoch 3100; error=0.254\n",
      "epoch 3150; error=0.243\n",
      "epoch 3200; error=0.232\n",
      "epoch 3250; error=0.221\n",
      "epoch 3300; error=0.212\n",
      "epoch 3350; error=0.203\n",
      "epoch 3400; error=0.195\n",
      "epoch 3450; error=0.187\n",
      "epoch 3500; error=0.18\n",
      "epoch 3550; error=0.173\n",
      "epoch 3600; error=0.166\n",
      "epoch 3650; error=0.16\n",
      "epoch 3700; error=0.155\n",
      "epoch 3750; error=0.149\n",
      "epoch 3800; error=0.144\n",
      "epoch 3850; error=0.139\n",
      "epoch 3900; error=0.135\n",
      "epoch 3950; error=0.131\n",
      "epoch 4000; error=0.127\n",
      "epoch 4050; error=0.123\n",
      "epoch 4100; error=0.119\n",
      "epoch 4150; error=0.116\n",
      "epoch 4200; error=0.112\n",
      "epoch 4250; error=0.109\n",
      "epoch 4300; error=0.106\n",
      "epoch 4350; error=0.103\n",
      "epoch 4400; error=0.101\n",
      "epoch 4450; error=0.098\n",
      "epoch 4500; error=0.096\n",
      "epoch 4550; error=0.093\n",
      "epoch 4600; error=0.091\n",
      "epoch 4650; error=0.089\n",
      "epoch 4700; error=0.087\n",
      "epoch 4750; error=0.085\n",
      "epoch 4800; error=0.083\n",
      "epoch 4850; error=0.081\n",
      "epoch 4900; error=0.079\n",
      "epoch 4950; error=0.077\n",
      "epoch 5000; error=0.076\n",
      "epoch 5050; error=0.074\n",
      "epoch 5100; error=0.073\n",
      "epoch 5150; error=0.071\n",
      "epoch 5200; error=0.07\n",
      "epoch 5250; error=0.068\n",
      "epoch 5300; error=0.067\n",
      "epoch 5350; error=0.066\n",
      "epoch 5400; error=0.065\n",
      "epoch 5450; error=0.063\n",
      "epoch 5500; error=0.062\n",
      "epoch 5550; error=0.061\n",
      "epoch 5600; error=0.06\n",
      "epoch 5650; error=0.059\n",
      "epoch 5700; error=0.058\n",
      "epoch 5750; error=0.057\n",
      "epoch 5800; error=0.056\n",
      "epoch 5850; error=0.055\n",
      "epoch 5900; error=0.054\n",
      "epoch 5950; error=0.054\n",
      "epoch 6000; error=0.053\n",
      "epoch 6050; error=0.052\n",
      "epoch 6100; error=0.051\n",
      "epoch 6150; error=0.05\n",
      "epoch 6200; error=0.05\n",
      "epoch 6250; error=0.049\n",
      "epoch 6300; error=0.048\n",
      "epoch 6350; error=0.047\n",
      "epoch 6400; error=0.047\n",
      "epoch 6450; error=0.046\n",
      "epoch 6500; error=0.045\n",
      "epoch 6550; error=0.045\n",
      "epoch 6600; error=0.044\n",
      "epoch 6650; error=0.044\n",
      "epoch 6700; error=0.043\n",
      "epoch 6750; error=0.042\n",
      "epoch 6800; error=0.042\n",
      "epoch 6850; error=0.041\n",
      "epoch 6900; error=0.041\n",
      "epoch 6950; error=0.04\n",
      "epoch 7000; error=0.04\n",
      "epoch 7050; error=0.039\n",
      "epoch 7100; error=0.039\n",
      "epoch 7150; error=0.038\n",
      "epoch 7200; error=0.038\n",
      "epoch 7250; error=0.038\n",
      "epoch 7300; error=0.037\n",
      "epoch 7350; error=0.037\n",
      "epoch 7400; error=0.036\n",
      "epoch 7450; error=0.036\n",
      "epoch 7500; error=0.035\n",
      "epoch 7550; error=0.035\n",
      "epoch 7600; error=0.035\n",
      "epoch 7650; error=0.034\n",
      "epoch 7700; error=0.034\n",
      "epoch 7750; error=0.034\n",
      "epoch 7800; error=0.033\n",
      "epoch 7850; error=0.033\n",
      "epoch 7900; error=0.033\n",
      "epoch 7950; error=0.032\n",
      "epoch 8000; error=0.032\n",
      "epoch 8050; error=0.032\n",
      "epoch 8100; error=0.031\n",
      "epoch 8150; error=0.031\n",
      "epoch 8200; error=0.031\n",
      "epoch 8250; error=0.03\n",
      "epoch 8300; error=0.03\n",
      "epoch 8350; error=0.03\n",
      "epoch 8400; error=0.029\n",
      "epoch 8450; error=0.029\n",
      "epoch 8500; error=0.029\n",
      "epoch 8550; error=0.029\n",
      "epoch 8600; error=0.028\n",
      "epoch 8650; error=0.028\n",
      "epoch 8700; error=0.028\n",
      "epoch 8750; error=0.028\n",
      "epoch 8800; error=0.027\n",
      "epoch 8850; error=0.027\n",
      "epoch 8900; error=0.027\n",
      "epoch 8950; error=0.027\n",
      "epoch 9000; error=0.026\n",
      "epoch 9050; error=0.026\n",
      "epoch 9100; error=0.026\n",
      "epoch 9150; error=0.026\n",
      "epoch 9200; error=0.026\n",
      "epoch 9250; error=0.025\n",
      "epoch 9300; error=0.025\n",
      "epoch 9350; error=0.025\n",
      "epoch 9400; error=0.025\n",
      "epoch 9450; error=0.025\n",
      "epoch 9500; error=0.024\n",
      "epoch 9550; error=0.024\n",
      "epoch 9600; error=0.024\n",
      "epoch 9650; error=0.024\n",
      "epoch 9700; error=0.024\n",
      "epoch 9750; error=0.023\n",
      "epoch 9800; error=0.023\n",
      "epoch 9850; error=0.023\n",
      "epoch 9900; error=0.023\n",
      "epoch 9950; error=0.023\n",
      "Final result:\n",
      " Input: [1. 1.]\n",
      " Output: 0.074\n",
      " Target: 0.0\n",
      "\n",
      "Final result:\n",
      " Input: [0. 1.]\n",
      " Output: 0.915\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 0.]\n",
      " Output: 0.94\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [0. 0.]\n",
      " Output: 0.079\n",
      " Target: 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same input tensor X and new labels y for xor\n",
    "Y_xor = torch.tensor([0.,1.,1.,0.])\n",
    "N = X.shape[0] # number of input patterns\n",
    "\n",
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w_34 = torch.randn(2,requires_grad=True) # [size 2] tensor representing [w_3,w_4]\n",
    "w_012 = torch.randn(3,requires_grad=True) # [size 3] tensor representing [w_0,w_1,w_2]\n",
    "b_0 = torch.randn(1,requires_grad=True) # [size 1] tensor\n",
    "b_1 = torch.randn(1,requires_grad=True) # [size 1] tensor\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 10000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x_pat = X[p,:] # input pattern\n",
    "        # Compute the output of hidden neuron h\n",
    "        # e.g., two lines like the following\n",
    "        #  net_h = ...\n",
    "        #  h = ...\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        net_h = torch.dot(x_pat,w_34)+b_0\n",
    "        h = g_logistic(net_h)\n",
    "        #raise Exception('Replace with your code.')                  \n",
    "        \n",
    "        # Compute the output of neuron yhat\n",
    "        # e.g., two lines like the following\n",
    "        #  net_y = ...\n",
    "        #  yhat = ...\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        x_h = torch.cat((x_pat,h))\n",
    "        net_y = torch.dot(x_h,w_012)+b_1\n",
    "        yhat = g_logistic(net_y)\n",
    "        #raise Exception('Replace with your code.')                     \n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_xor[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # print output if this is the last epoch\n",
    "        if (e == nepochs-1):\n",
    "            print(\"Final result:\")\n",
    "            print_forward(x_pat,yhat,y)\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x_pat,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  should include at least these 4 lines (helper lines may be useful)\n",
    "            #    w_34_grad = ...  \n",
    "            #    b_0_grad = ...\n",
    "            #    w_012_grad = ...\n",
    "            #    b_1_grad = ...\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.\n",
    "            w_34_grad = torch.tensor([2*(yhat-y)\n",
    "                                     *g_logistic(net_y)\n",
    "                                     *(1-g_logistic(net_y))*w_012[2]\n",
    "                                       *g_logistic(net_h)*(1-g_logistic(net_h))\n",
    "                                      *x_pat[0],\n",
    "                                    2*(yhat-y)\n",
    "                                     *g_logistic(net_y)\n",
    "                                     *(1-g_logistic(net_y))*w_012[2]\n",
    "                                       *g_logistic(net_h)*(1-g_logistic(net_h))\n",
    "                                      *x_pat[1]])\n",
    "            b_0_grad = torch.tensor([2*(yhat-y)\n",
    "                                     *g_logistic(net_y)\n",
    "                                     *(1-g_logistic(net_y))*w_012[2]\n",
    "                                     *g_logistic(net_h)*(1-g_logistic(net_h))])\n",
    "            w_012_grad = torch.tensor([2*(yhat-y)*g_logistic(net_y)\n",
    "                                       *(1-g_logistic(net_y))\n",
    "                                       *x_pat[0],\n",
    "                                       2*(yhat-y)*g_logistic(net_y)\n",
    "                                       *(1-g_logistic(net_y))*x_pat[1],\n",
    "                                      2*(yhat-y)*g_logistic(net_y)\n",
    "                                       *(1-g_logistic(net_y))*h])\n",
    "            b_1_grad = torch.tensor([2*(yhat-y)*g_logistic(net_y)\n",
    "                                     *(1-g_logistic(net_y))])\n",
    "            #raise Exception('Replace with your code.')                      \n",
    "        if verbose:\n",
    "            print(\" Grad for w_34 and b_0\")\n",
    "            print_grad(w_34_grad.numpy(),b_0_grad.numpy())\n",
    "            print(\" Grad for w_012 and b_1\")\n",
    "            print_grad(w_012_grad.numpy(),b_1_grad.numpy())\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print(\" Grad for w_34 and b_0\")\n",
    "            print_grad(w_34.grad.numpy(),b_0.grad.numpy())\n",
    "            print(\" Grad for w_012 and b_1\")\n",
    "            print_grad(w_012.grad.numpy(),b_1.grad.numpy())\n",
    "            print(\"\")\n",
    "        w_34.grad.zero_() # clear PyTorch's gradient\n",
    "        b_0.grad.zero_()\n",
    "        w_012.grad.zero_()\n",
    "        b_1.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            # Four lines of the form\n",
    "            # w_34 -= ...\n",
    "            # b_0 -= ...\n",
    "            # w_012 -= ...\n",
    "            # b_1 -= ...\n",
    "            w_34 -= alpha*w_34_grad\n",
    "            b_0 -= alpha*b_0_grad\n",
    "            w_012 -= alpha*w_012_grad\n",
    "            b_1 -= alpha*b_1_grad\n",
    "            #raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "\n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (XOR)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 6 (10 points) </h3>\n",
    "<br>\n",
    "After running your XOR network, print the values of the learned weights and biases. Your job now is to describe the solution that the network has learned. How does it work? Walk through each input pattern to describe how the network computes the right answer (if it does). See discussion in lecture for an example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.6031, -6.9131], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.1064,  5.0970, 11.1660], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w_012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.7279], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(b_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7180], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(b_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $x_0$ and $x_1$ are 0, the hidden layer gets no input. Since the bias $b_0$ for the hidden layer is negative, the hidden layer doesn't get activated, which leads to the output being equal to the output layer's bias $b_1$, which is negative. As a result, the output is not activated ($=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $x_0=0$ and $x_1=1$, $x_0$ will not send any signal to the hidden layer, but $x_1$ will with a negative weight, so the hidden layer is not activated. Therefore the output will only receive input directly from $x_1$ with a weight of $w_1=5.0970 > b_1 = -2.7180$, so the output will be activated ($=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $x_0=1$ and $x_1=0$, $x_1$ will not send any signal to the hidden layer, but $x_0$ will with a negative weight, so the hidden layer is not activated. Therefore the output will only receive input directly from $x_1$ with a weight of $w_1=5.0970 > b_1 = -2.7180$, so the output will be activated ($=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $x_0=1$ and $x_1=0$, $x_1$ will not send any signal to the hidden layer, but $x_0$ will with a weight of $w_3 = 6.6031 > b_0=-3.7279$, so the hidden layer is activated ($=1$). The hidden layer sends a signal with $w_2=11.1660 > |w_0| + |b_1| = |-5.1064| + |-2.7180|$ to the output layer, so the output is activated ($=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $x_0=1 = x_1=1$, they both send signals to the hidden layer but with approximately equal weights of opposite signs, which will cancel each other out ($w_0=-5.1064$, $w_1=5.0970$), so the hidden layer will not be activated. $x_0$ and $x_1$ will also send signals directly to the output with approximately the same weights of opposite signs, which again will cancel out. The only input then that the output layer will receive is $b_1=-2.7180$ which is negative, so the output layer is not activated ($=0$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
